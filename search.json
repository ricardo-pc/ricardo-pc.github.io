[
  {
    "objectID": "coursework.html",
    "href": "coursework.html",
    "title": "Coursework",
    "section": "",
    "text": "Coursework highlights the tools and theory I use for data science, with an emphasis on modeling, decision-making, and scalable computation.\n\n\n\n\n\n\n\n\n\nApplied machine learning for real-world decision-making. Focuses on model evaluation under uncertainty, causal inference methods, and practical deployment of ML systems for high-impact decisions.\n\nPython Applied ML Causal Inference\n\n\n\n\n\n\n\n\n\n\n\nComprehensive regression theory from OLS and Gauss-Markov theorem to GLMs and generalized estimating equations. Covers heteroskedasticity, cluster-robust inference, model selection, shrinkage methods (Ridge/LASSO), and extensions to categorical and count outcomes.\n\nPython Regression GLMs\n\n\n\n\n\n\n\n\n\n\n\nProbabilistic machine learning theory covering empirical risk minimization, complexity bounds, and uniform laws. Emphasizes model evaluation via cross-validation, regularization schemes, classification with proxy losses, and connections between linear models and sophisticated ML methods.\n\nPython Deep Learning Optimization\n\n\n\n\n\n\n\n\n\n\n\nTheoretical foundations and applications of ML from problem formulation to model deployment. Covers unsupervised learning (k-means, PCA), regression and classification, deep learning architectures (CNNs, RNNs, transformers), and generative models (LLMs, GANs). Includes PyTorch implementation and backpropagation.\n\nPython ML Theory Algorithms\n\n\n\n\n\n\n\n\n\n\n\nBayesian inference framework including prior elicitation, conjugate analysis, and hierarchical modeling. Focuses on MCMC methods (Gibbs sampling, Metropolis-Hastings), modern computational techniques (HMC, variational inference), and applications to complex statistical models.\n\nPython MCMC Hierarchical Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure-theoretic probability, convergence concepts, laws of large numbers, central limit theorems, and foundational theory for statistical inference.\n\nTheory Measure Theory Convergence\n\n\n\n\n\n\n\n\n\n\n\nStatistical inference theory including sufficiency, exponential families, hypothesis testing, asymptotic theory, and decision theory foundations.\n\nTheory Inference Asymptotics\n\n\n\n\n\n\n\n\n\n\n\nStatistical computing and data science using Python. Covers UNIX/bash scripting, version control, data manipulation, text processing, object-oriented programming, parallel processing, databases and big data, computer arithmetic, simulation and Monte Carlo methods, numerical linear algebra, and optimization.\n\nPython UNIX/Bash Parallel Computing Numerical Methods"
  },
  {
    "objectID": "coursework.html#uc-berkeley-m.a.-in-statistics",
    "href": "coursework.html#uc-berkeley-m.a.-in-statistics",
    "title": "Coursework",
    "section": "",
    "text": "Coursework highlights the tools and theory I use for data science, with an emphasis on modeling, decision-making, and scalable computation.\n\n\n\n\n\n\n\n\n\nApplied machine learning for real-world decision-making. Focuses on model evaluation under uncertainty, causal inference methods, and practical deployment of ML systems for high-impact decisions.\n\nPython Applied ML Causal Inference\n\n\n\n\n\n\n\n\n\n\n\nComprehensive regression theory from OLS and Gauss-Markov theorem to GLMs and generalized estimating equations. Covers heteroskedasticity, cluster-robust inference, model selection, shrinkage methods (Ridge/LASSO), and extensions to categorical and count outcomes.\n\nPython Regression GLMs\n\n\n\n\n\n\n\n\n\n\n\nProbabilistic machine learning theory covering empirical risk minimization, complexity bounds, and uniform laws. Emphasizes model evaluation via cross-validation, regularization schemes, classification with proxy losses, and connections between linear models and sophisticated ML methods.\n\nPython Deep Learning Optimization\n\n\n\n\n\n\n\n\n\n\n\nTheoretical foundations and applications of ML from problem formulation to model deployment. Covers unsupervised learning (k-means, PCA), regression and classification, deep learning architectures (CNNs, RNNs, transformers), and generative models (LLMs, GANs). Includes PyTorch implementation and backpropagation.\n\nPython ML Theory Algorithms\n\n\n\n\n\n\n\n\n\n\n\nBayesian inference framework including prior elicitation, conjugate analysis, and hierarchical modeling. Focuses on MCMC methods (Gibbs sampling, Metropolis-Hastings), modern computational techniques (HMC, variational inference), and applications to complex statistical models.\n\nPython MCMC Hierarchical Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure-theoretic probability, convergence concepts, laws of large numbers, central limit theorems, and foundational theory for statistical inference.\n\nTheory Measure Theory Convergence\n\n\n\n\n\n\n\n\n\n\n\nStatistical inference theory including sufficiency, exponential families, hypothesis testing, asymptotic theory, and decision theory foundations.\n\nTheory Inference Asymptotics\n\n\n\n\n\n\n\n\n\n\n\nStatistical computing and data science using Python. Covers UNIX/bash scripting, version control, data manipulation, text processing, object-oriented programming, parallel processing, databases and big data, computer arithmetic, simulation and Monte Carlo methods, numerical linear algebra, and optimization.\n\nPython UNIX/Bash Parallel Computing Numerical Methods"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Personal Website"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Here are some personal projects exploring data science, data engineering, and machine learning.\n\n\n\n\n\n\n\n\n\n\n\nEvolutionary Feature Selection: A Genetic Algorithm Package\n\n\n\nPython\n\nStatistical Computing\n\nMachine Learning\n\nSoftware Engineering\n\n\n\nBuilding a production-quality Python package for automated variable selection using evolutionary computation principles.\n\n\n\nDec 15, 2024\n\n\n\n\n\n\n\n\n\n\nDigital Water: Predicting a Crisis Before It Happened\n\n\n\nMachine Learning\n\nPython\n\nTime Series\n\nImpact\n\n\n\nHow our ML forecasting system predicted Monterrey‚Äôs water crisis two years before 5 million people faced severe rationing.\n\n\n\nAug 1, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "selected-work/stat243.html",
    "href": "selected-work/stat243.html",
    "title": "STAT 243 ‚Äî Statistical Computing",
    "section": "",
    "text": "Showcase of projects and assignments from STAT 243."
  },
  {
    "objectID": "selected-work/stat243.html#highlights",
    "href": "selected-work/stat243.html#highlights",
    "title": "STAT 243 ‚Äî Statistical Computing",
    "section": "Highlights",
    "text": "Highlights\n\nIn progress."
  },
  {
    "objectID": "selected-work/stat243.html#artifacts",
    "href": "selected-work/stat243.html#artifacts",
    "title": "STAT 243 ‚Äî Statistical Computing",
    "section": "Artifacts",
    "text": "Artifacts\n\nIn progress."
  },
  {
    "objectID": "posts/digital-water/index.html",
    "href": "posts/digital-water/index.html",
    "title": "Digital Water: Predicting a Crisis Before It Happened",
    "section": "",
    "text": "NoteThe Story at a Glance\n\n\n\nIn 2020, our team built an AI system to forecast water demand for my hometown of Monterrey, Mexico. We won 2nd place globally at the Atos IT Challenge, the first Mexican team ever to achieve this distinction in the competition‚Äôs 10-year history.\nTwo years later, our predictions came true. Monterrey plunged into a severe water crisis, forcing 5 million residents to survive on minimal daily water allocations for nearly two years."
  },
  {
    "objectID": "posts/digital-water/index.html#what-if-you-could-see-a-crisis-coming",
    "href": "posts/digital-water/index.html#what-if-you-could-see-a-crisis-coming",
    "title": "Digital Water: Predicting a Crisis Before It Happened",
    "section": "What If You Could See a Crisis Coming?",
    "text": "What If You Could See a Crisis Coming?\nBetween November 2019 and July 2020, my team developed Digital Water, a machine learning platform to forecast water demand and capacity at the city level. Working with real data from Monterrey‚Äôs official water utility (SADM), we built LSTM neural networks that could predict daily water consumption and available dam capacity.\nAt the time, we were focused on winning an international competition. We didn‚Äôt realize we were documenting the early warning signs of a catastrophe."
  },
  {
    "objectID": "posts/digital-water/index.html#the-challenge",
    "href": "posts/digital-water/index.html#the-challenge",
    "title": "Digital Water: Predicting a Crisis Before It Happened",
    "section": "The Challenge",
    "text": "The Challenge\nMonterrey is Mexico‚Äôs third-largest metro area and a major industrial hub. The city‚Äôs water system serves over 5 million people, drawing from a network of dams and underground sources. But water scarcity was becoming more frequent, and utilities were operating reactively, responding to shortages rather than preventing them.\nOur goal: Build a forecasting system that could shift water management from reactive to preventative, giving utilities early warning signals before reaching critical thresholds."
  },
  {
    "objectID": "posts/digital-water/index.html#our-solution",
    "href": "posts/digital-water/index.html#our-solution",
    "title": "Digital Water: Predicting a Crisis Before It Happened",
    "section": "Our Solution",
    "text": "Our Solution\nWe developed an end-to-end machine learning pipeline:\n\nData Engineering\n\nCollected and cleaned multi-source time-series data from SADM\nAligned daily consumption records with dam capacity measurements\nCreated comprehensive field dictionaries and documentation\n\n\n\nPredictive Models\n\nTrained two LSTM neural networks (one for demand, one for capacity)\nForecasted daily water demand and available capacity\nCalculated ‚Äúdays-to-threshold‚Äù estimates for safe operating levels\nImplemented in Python using TensorFlow\n\n\n\nDeployment\n\nBuilt an interactive web dashboard (hosted on Heroku)\nCreated Google Colab notebooks for accessibility and reproducibility\nProduced technical documentation and presentation materials\n\n\n\n\n\n\n\nTipKey Technologies\n\n\n\nPython ‚Ä¢ TensorFlow ‚Ä¢ LSTM Networks ‚Ä¢ Time Series Forecasting ‚Ä¢ Google Colab ‚Ä¢ Heroku"
  },
  {
    "objectID": "posts/digital-water/index.html#recognition",
    "href": "posts/digital-water/index.html#recognition",
    "title": "Digital Water: Predicting a Crisis Before It Happened",
    "section": "Recognition",
    "text": "Recognition\n\n\nüèÜ 2nd Place Globally Atos IT Challenge 2020\n\n\nüá≤üáΩ First Mexican Team In competition‚Äôs 10-year history\n\n\nüë• 5M People Impacted By predicted crisis\n\n\nWe presented our work to the Atos CEO and Scientific Board, competing against thousands of international teams."
  },
  {
    "objectID": "posts/digital-water/index.html#what-happened-next-the-crisis",
    "href": "posts/digital-water/index.html#what-happened-next-the-crisis",
    "title": "Digital Water: Predicting a Crisis Before It Happened",
    "section": "What Happened Next: The Crisis",
    "text": "What Happened Next: The Crisis\nIn 2022, everything our models predicted came to pass.\nMonterrey experienced a devastating water crisis that lasted nearly two years. The city‚Äôs 5 million residents were reduced to minimal daily water allocations. Neighborhoods received water only a few hours per week. Businesses struggled. The crisis validated our forecasting approach‚Äîbut also showed the human cost of delayed action.\n\n\n\n\n\n\nWarningTimeline\n\n\n\n2019-2020: Built forecasting models with SADM data\nJuly 2020: Won 2nd place at Atos IT Challenge\n2022: Water crisis begins‚Äîpredictions validated\n2022-2024: Nearly two years of severe rationing for 5M residents\n2024: Crisis subsides\n\n\nThe crisis highlighted how proactive forecasting can shift utilities from reactive operations to preventative planning‚Äîand why such systems need to be deployed, not just built."
  },
  {
    "objectID": "posts/digital-water/index.html#technical-details",
    "href": "posts/digital-water/index.html#technical-details",
    "title": "Digital Water: Predicting a Crisis Before It Happened",
    "section": "Technical Details",
    "text": "Technical Details\nFor those interested in the implementation:\n\nModels: Two LSTM neural networks (demand + capacity forecasting)\nData: Official records from Agua y Drenaje de Monterrey (SADM)\nFeatures: Multi-variate time series including consumption patterns, dam levels, and seasonal factors\nOutputs: Daily forecasts + ‚Äúdays-to-threshold‚Äù risk estimates\nCode: Open-source notebooks in Google Colab\nDocumentation: Complete technical report and research summaries"
  },
  {
    "objectID": "posts/digital-water/index.html#links-resources",
    "href": "posts/digital-water/index.html#links-resources",
    "title": "Digital Water: Predicting a Crisis Before It Happened",
    "section": "Links & Resources",
    "text": "Links & Resources\n\nGitHub Repository ‚Äî Code, notebooks, and documentation\nDemo Video ‚Äî Platform walkthrough"
  },
  {
    "objectID": "posts/digital-water/index.html#reflection",
    "href": "posts/digital-water/index.html#reflection",
    "title": "Digital Water: Predicting a Crisis Before It Happened",
    "section": "Reflection",
    "text": "Reflection\nThis project was completed in 2020, and the notebooks remain as-is in the repository. Looking back (5 years later) with more experience in ML, I would redesign the data pipeline, modeling aproach, stability and predictability of the model and scalability. But the core lesson remains: predictive models only create impact when they‚Äôre deployed and acted upon.\nThe crisis our system forecasted was preventable. The data was there. The models worked. What was missing was the infrastructure and institutional will to act on early warnings.\n\nBuilt between November 2019 and July 2020 as part of the Atos IT Challenge."
  },
  {
    "objectID": "posts/genetic-algorithm/index.html",
    "href": "posts/genetic-algorithm/index.html",
    "title": "Evolutionary Feature Selection: A Genetic Algorithm Package",
    "section": "",
    "text": "NoteProject Overview\n\n\n\nA collaborative final project for STAT 243 (Statistical Computing) implementing a genetic algorithm for automated variable selection. Built as a production-ready Python package with comprehensive testing, modular architecture, and support for multiple regression methods.\nTeam: Collaborative project with two classmates ‚Ä¢ Duration: 4 weeks ‚Ä¢ Language: Python"
  },
  {
    "objectID": "posts/genetic-algorithm/index.html#the-challenge-variable-selection-at-scale",
    "href": "posts/genetic-algorithm/index.html#the-challenge-variable-selection-at-scale",
    "title": "Evolutionary Feature Selection: A Genetic Algorithm Package",
    "section": "The Challenge: Variable Selection at Scale",
    "text": "The Challenge: Variable Selection at Scale\nIn predictive modeling, choosing the right set of variables is crucial. Too few predictors and you miss important signals. Too many and you overfit, harming generalization. When facing datasets with dozens or hundreds of potential predictors, exhaustive search becomes computationally infeasible‚Äîexploring all possible combinations grows exponentially.\nThe assignment: Build a Python package implementing genetic algorithms for variable selection that balances predictive accuracy against model complexity."
  },
  {
    "objectID": "posts/genetic-algorithm/index.html#what-are-genetic-algorithms",
    "href": "posts/genetic-algorithm/index.html#what-are-genetic-algorithms",
    "title": "Evolutionary Feature Selection: A Genetic Algorithm Package",
    "section": "What Are Genetic Algorithms?",
    "text": "What Are Genetic Algorithms?\nGenetic algorithms mimic natural selection to solve optimization problems. Instead of evaluating every possible solution, they:\n\nInitialize a population of random variable combinations (chromosomes)\nEvaluate fitness using cross-validated predictive performance\nSelect the best-performing combinations as parents\nCrossover parent chromosomes to create offspring\nMutate randomly to explore new solutions\nEvolve iteratively across generations toward optimal feature sets\n\nThis evolutionary approach efficiently searches massive solution spaces by learning from successful combinations."
  },
  {
    "objectID": "posts/genetic-algorithm/index.html#our-solution-the-ga-package",
    "href": "posts/genetic-algorithm/index.html#our-solution-the-ga-package",
    "title": "Evolutionary Feature Selection: A Genetic Algorithm Package",
    "section": "Our Solution: The GA Package",
    "text": "Our Solution: The GA Package\nWe developed a flexible, well-tested Python package that brings genetic algorithms to practical variable selection tasks.\n\nCore Features\n\n\n\n\n\n\nTipKey Capabilities\n\n\n\nMultiple Model Types ‚Ä¢ Linear Regression ‚Ä¢ LASSO Regression ‚Ä¢ Decision Trees\nFlexible Selection ‚Ä¢ Rank-based parent selection ‚Ä¢ Tournament selection\nCrossover Methods ‚Ä¢ Single-point recombination ‚Ä¢ Double-point recombination\nPenalized Optimization ‚Ä¢ Balance accuracy vs.¬†complexity ‚Ä¢ Adjustable sparsity penalty\n\n\n\n\nArchitecture & Design\nThe package follows software engineering best practices:\nModular Design - Core select() function as primary interface - Discrete methods for initialization, fitness evaluation, selection, crossover, mutation - Clear separation of concerns across components\nRobust Testing - Unit tests for individual genetic operators - Integration tests for full algorithm - pytest framework with automated GitHub Actions CI/CD - Validation against known benchmarks\nPerformance Optimization - Vectorized operations within generations - Parallel cross-validation for fitness evaluation - Efficient NumPy/scikit-learn implementations\nProfessional Documentation - Comprehensive README with examples - Docstrings following Python standards - Clear usage demonstrations\n\n\nTechnical Implementation\nThe algorithm evaluates fitness using cross-validated R¬≤ with optional complexity penalties:\n\\[\n\\text{Penalized Fitness} = R^2 - \\lambda \\cdot f\n\\]\nwhere: - \\(R^2\\) = cross-validated coefficient of determination - \\(\\lambda\\) = penalty parameter (controls sparsity) - \\(f\\) = fraction of total predictors selected\nThis formulation prevents overfitting while encouraging parsimonious models."
  },
  {
    "objectID": "posts/genetic-algorithm/index.html#development-process",
    "href": "posts/genetic-algorithm/index.html#development-process",
    "title": "Evolutionary Feature Selection: A Genetic Algorithm Package",
    "section": "Development Process",
    "text": "Development Process\n\nCollaborative Workflow\nWorking as a three-person team, we:\n\nMapped architecture before writing code‚Äîdefining modular components and interfaces\nDivided responsibilities across initialization, genetic operators, fitness evaluation, and testing\nUsed Git workflows with branches and pull requests for code review\nCross-tested components ‚Äî each person‚Äôs code was reviewed and tested by teammates\nIntegrated continuously via automated testing on GitHub Actions\n\n\n\nTechnical Requirements Met\n\n\n‚úì Modular Architecture Clean separation of genetic operators\n\n\n‚úì Formal Testing pytest suite with CI/CD automation\n\n\n‚úì Performance Vectorization & parallel processing\n\n\n‚úì Documentation Complete README & docstrings"
  },
  {
    "objectID": "posts/genetic-algorithm/index.html#results-applications",
    "href": "posts/genetic-algorithm/index.html#results-applications",
    "title": "Evolutionary Feature Selection: A Genetic Algorithm Package",
    "section": "Results & Applications",
    "text": "Results & Applications\nThe package successfully identifies important predictors across various scenarios:\n\nBaseball statistics (original benchmark dataset)\nSimulated data with known true predictors\nReal-world datasets with complex predictor relationships\n\nPerformance highlights: - Efficiently handles datasets with 20+ potential predictors - Balances predictive accuracy with model sparsity - Reasonable runtime under default settings (~minutes for typical problems) - Supports experimentation with different GA parameters"
  },
  {
    "objectID": "posts/genetic-algorithm/index.html#technologies-tools",
    "href": "posts/genetic-algorithm/index.html#technologies-tools",
    "title": "Evolutionary Feature Selection: A Genetic Algorithm Package",
    "section": "Technologies & Tools",
    "text": "Technologies & Tools\nCore Stack - Python ‚Äî Primary implementation language - scikit-learn ‚Äî Regression models and cross-validation - NumPy ‚Äî Vectorized numerical operations - pytest ‚Äî Testing framework\nDevelopment Tools - Git/GitHub ‚Äî Version control and collaboration - GitHub Actions ‚Äî Automated testing and CI/CD - VSCode ‚Äî Development environment"
  },
  {
    "objectID": "posts/genetic-algorithm/index.html#key-takeaways",
    "href": "posts/genetic-algorithm/index.html#key-takeaways",
    "title": "Evolutionary Feature Selection: A Genetic Algorithm Package",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nSoftware Engineering in Statistics\nThis project reinforced the importance of treating statistical code as production software:\n\nModularity matters ‚Äî Breaking algorithms into discrete, testable components\nTesting is essential ‚Äî Formal tests catch bugs and ensure reliability\nDocumentation enables use ‚Äî Clear READMEs and docstrings make code accessible\nPerformance counts ‚Äî Vectorization and parallelization for real-world speed\nCollaboration requires structure ‚Äî Git workflows and code review maintain quality\n\n\n\nEvolutionary Computation Insights\nImplementing genetic algorithms from scratch provided deep understanding:\n\nHow crossover and mutation balance exploration vs.¬†exploitation\nThe impact of selection pressure on convergence speed\nTrade-offs between population size and number of generations\nImportance of fitness function design for optimization success"
  },
  {
    "objectID": "posts/genetic-algorithm/index.html#links-resources",
    "href": "posts/genetic-algorithm/index.html#links-resources",
    "title": "Evolutionary Feature Selection: A Genetic Algorithm Package",
    "section": "Links & Resources",
    "text": "Links & Resources\n\nGitHub Repository ‚Äî Complete source code and documentation\nProject Specification ‚Äî Original assignment requirements"
  },
  {
    "objectID": "posts/genetic-algorithm/index.html#reflection",
    "href": "posts/genetic-algorithm/index.html#reflection",
    "title": "Evolutionary Feature Selection: A Genetic Algorithm Package",
    "section": "Reflection",
    "text": "Reflection\nBuilding a production-quality statistical computing package taught me that good statistical software requires more than correct algorithms‚Äîit demands thoughtful architecture, comprehensive testing, performance optimization, and clear documentation.\nThe collaborative aspect was equally valuable. Working as a team meant negotiating interfaces, reviewing each other‚Äôs code, and maintaining consistency across components. These skills translate directly to professional data science and software engineering.\n\nCompleted December 2024 as the final project for STAT 243: Statistical Computing at UC Berkeley."
  },
  {
    "objectID": "selected-work/ml-current.html",
    "href": "selected-work/ml-current.html",
    "title": "Current ML Courses ‚Äî Projects",
    "section": "",
    "text": "Section in progress."
  },
  {
    "objectID": "selected-work/ml-current.html#highlights",
    "href": "selected-work/ml-current.html#highlights",
    "title": "Current ML Courses ‚Äî Projects",
    "section": "Highlights",
    "text": "Highlights\n\nIn progress."
  },
  {
    "objectID": "selected-work/ml-current.html#artifacts",
    "href": "selected-work/ml-current.html#artifacts",
    "title": "Current ML Courses ‚Äî Projects",
    "section": "Artifacts",
    "text": "Artifacts\n\nIn progress."
  },
  {
    "objectID": "selected-work/stat201.html",
    "href": "selected-work/stat201.html",
    "title": "STAT 201A/201B ‚Äî Advanced Probability and Statistics",
    "section": "",
    "text": "Showcase of assignments from STAT 201A/B."
  },
  {
    "objectID": "selected-work/stat201.html#highlights",
    "href": "selected-work/stat201.html#highlights",
    "title": "STAT 201A/201B ‚Äî Advanced Probability and Statistics",
    "section": "Highlights",
    "text": "Highlights\n\nIn progress."
  },
  {
    "objectID": "selected-work/stat201.html#artifacts",
    "href": "selected-work/stat201.html#artifacts",
    "title": "STAT 201A/201B ‚Äî Advanced Probability and Statistics",
    "section": "Artifacts",
    "text": "Artifacts\n\nIn progress."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ricardo P√©rez Castillo",
    "section": "",
    "text": "I‚Äôm a Statistics M.A.¬†student at UC Berkeley preparing for a career in data science, focused on turning complex data into actionable insights.\nI bring a strong data analytics, data engineering, and consulting background from McKinsey and Deloitte. There, I built large-scale ETL pipelines, data warehouse architectures, and extracted insights from large datasets, and I now apply those skills to modeling, experimentation, and decision-focused analytics.\nCurrent interests:\nExplore my projects or download my CV."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Ricardo P√©rez Castillo",
    "section": "Education",
    "text": "Education\nUniversity of California, Berkeley | Berkeley, CA\nM.A.¬†in Statistics | Aug 2025 - Present\nTecnol√≥gico de Monterrey | Monterrey, Mexico\nB.S. in Chemical Engineering | Jan 2015 - Dec 2019"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Download the PDF: Curriculum_RicardoPerezCastillo.pdf"
  }
]